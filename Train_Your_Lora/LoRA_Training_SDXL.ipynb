{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YhRzp4wwrkBZ",
      "metadata": {
        "id": "YhRzp4wwrkBZ"
      },
      "source": [
        "# üîß LoRA Training (Stable Diffusion XL (SDXL)) ‚Äî Google Colab\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tHs1GHFpa-tZ",
      "metadata": {
        "id": "tHs1GHFpa-tZ"
      },
      "source": [
        "### Step - 1 : Environment Setup ‚Äî GPU (A100 Reccomemded (36 GB+ Of VRAM) ) & Drive Mount\n",
        "\n",
        "Before installing dependencies, we need to:  \n",
        "\n",
        "- **Check GPU** ‚Äî make sure Colab is running with a GPU.  \n",
        "- **Mount Google Drive** ‚Äî to save datasets, models, and outputs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k6nzBB_mrkBd",
      "metadata": {
        "id": "k6nzBB_mrkBd"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi || echo \"No GPU detected ‚Äî set Colab runtime to GPU.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kY96rsKYrkBf",
      "metadata": {
        "id": "kY96rsKYrkBf"
      },
      "outputs": [],
      "source": [
        "# STEP 1 ‚Äî Mount Google Drive (stores datasets/models/outputs)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unmount Google Drive if needed\n",
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To Move Your Data from Gdrive\n",
        "# !cp -r \"/content/drive/MyDrive/data\" /content/aigenmodel/10_aigenmodel"
      ],
      "metadata": {
        "id": "sLZnfFEPbCdn"
      },
      "id": "sLZnfFEPbCdn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Gv9y1TxzfOBC",
      "metadata": {
        "id": "Gv9y1TxzfOBC"
      },
      "source": [
        "## Step 2 : Dependency Installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install xFormers built for the existing torch 2.8.0 + cu126 without changing torch\n",
        "!pip install --upgrade --no-deps xformers\n",
        "\n",
        "# 2) Install the rest of the training deps\n",
        "!pip install --upgrade bitsandbytes pytorch-lightning prodigyopt==1.0 lion-pytorch==0.0.6 ftfy easygui voluptuous opencv-python"
      ],
      "metadata": {
        "id": "YnyMLZekhYJa"
      },
      "id": "YnyMLZekhYJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency Check Here\n",
        "# opencv-python Missing is OK Only for GUI\n",
        "\n",
        "import importlib\n",
        "\n",
        "libs_to_check = [\n",
        "    # Hugging Face core\n",
        "    \"torch\", \"torchvision\", \"torchaudio\",\n",
        "    \"transformers\", \"diffusers\", \"accelerate\",\n",
        "\n",
        "    # Memory/optimization\n",
        "    \"xformers\", \"bitsandbytes\", \"safetensors\", \"einops\",\n",
        "\n",
        "    # Training frameworks\n",
        "    \"pytorch_lightning\", \"prodigyopt\", \"lion_pytorch\",\n",
        "\n",
        "    # Data & preprocessing\n",
        "    \"opencv-python\", \"cv2\", \"PIL\", \"ftfy\", \"sentencepiece\", \"datasets\",\n",
        "\n",
        "    # Utils & viz\n",
        "    \"tensorboard\", \"huggingface_hub\", \"altair\", \"easygui\",\n",
        "    \"toml\", \"voluptuous\", \"imagesize\", \"rich\"\n",
        "]\n",
        "\n",
        "# special cases where version attribute differs\n",
        "special_version_attrs = {\n",
        "    \"cv2\": \"version\",\n",
        "    \"PIL\": \"PILLOW_VERSION\",  # old\n",
        "}\n",
        "\n",
        "for lib in libs_to_check:\n",
        "    try:\n",
        "        module_name = lib.replace(\"-\", \"_\")\n",
        "        module = importlib.import_module(module_name)\n",
        "\n",
        "        # try __version__, or special attributes, or unknown\n",
        "        version = getattr(module, \"__version__\", None)\n",
        "        if version is None:\n",
        "            # handle cv2, PIL, etc.\n",
        "            if lib in special_version_attrs:\n",
        "                version = getattr(module, special_version_attrs[lib], None)\n",
        "            elif lib == \"PIL\":\n",
        "                try:\n",
        "                    import PIL\n",
        "                    version = getattr(PIL, \"__version__\", None)\n",
        "                except Exception:\n",
        "                    version = None\n",
        "\n",
        "        print(f\"‚úÖ {lib} - {version if version else 'unknown'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {lib} - MISSING ({e.__class__.__name__})\")"
      ],
      "metadata": {
        "id": "qZH7cAOb28Bx"
      },
      "id": "qZH7cAOb28Bx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "TGjFh3XTrkBh",
      "metadata": {
        "id": "TGjFh3XTrkBh"
      },
      "source": [
        "## STEP 3 ‚Äî Choose Base Model\n",
        "You have **two options**:\n",
        "1. **Download SDXL from Hugging Face** (requires a free account + accepted license). Recommended for first-time users.\n",
        "2. **Point to a local `.safetensors`** you already have in Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p9bmcqeEjEqL",
      "metadata": {
        "id": "p9bmcqeEjEqL"
      },
      "outputs": [],
      "source": [
        "# B: Clone kohya-ss (sd-scripts) and install basic requirements\n",
        "\n",
        "!if [ ! -d \"/content/sd-scripts\" ]; then \\\n",
        "  git clone https://github.com/kohya-ss/sd-scripts.git /content/sd-scripts; \\\n",
        "else \\\n",
        "  echo \"‚úÖ sd-scripts already cloned at /content/sd-scripts\"; \\\n",
        "fi\n",
        "\n",
        "%cd /content/sd-scripts\n",
        "\n",
        "# !pip install -q -U pip\n",
        "# !pip install -q -r requirements.txt || echo \"‚ö†Ô∏è requirements install had issues; we‚Äôll fix specific packages later\"\n",
        "\n",
        "print(\"‚úÖ kohya repo ready at /content/sd-scripts (check any warnings above).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DZk5Lazz2sWj",
      "metadata": {
        "id": "DZk5Lazz2sWj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download, login\n",
        "\n",
        "# üîë Login with your HuggingFace token (must have accepted SDXL license)\n",
        "login()\n",
        "MODEL = \"sdxl-base-1.0\"\n",
        "MODEL_DIR = f\"/content/{MODEL}\" # Set our Path to Drive If needed\n",
        "\n",
        "# üì• Download SDXL 1.0 base model from StabilityAI\n",
        "\n",
        "# # Instead of downloading again, just check if files exist\n",
        "# import os\n",
        "# if not os.path.exists(f\"{MODEL_DIR}/sd_xl_base_1.0.safetensors\"):\n",
        "#     snapshot_download(\n",
        "#         repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "#         local_dir=MODEL_DIR,\n",
        "#         allow_patterns=[\"*.safetensors\",\"*.json\",\"tokenizer/*\",\"scheduler/*\",\"vae/*\",\"unet/config.json\",\"text_encoder/*\"]\n",
        "#     )\n",
        "# else:\n",
        "#     print(\"‚úÖ Model already exists in Drive. Skipping download.\")\n",
        "\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    local_dir=MODEL_DIR,\n",
        "    local_dir_use_symlinks=False,\n",
        "    allow_patterns=[\n",
        "        \"*.safetensors\",              # main weights\n",
        "        \"model_index.json\",           # model config\n",
        "        \"tokenizer/*\",\n",
        "        \"tokenizer/*\", \"tokenizer_2/*\",   # << include both!\n",
        "        \"text_encoder/*\", \"text_encoder_2/*\",# tokenizer files\n",
        "        \"scheduler/*\",                # schedulers\n",
        "        \"vae/*\",                      # VAE\n",
        "        \"unet/*\",                     # UNet\n",
        "        \"text_encoder/*\",             # text encoder\n",
        "        \"*.json\",                     # other configs\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ SDXL 1.0 Base download complete at /content/drive/MyDrive/SD/models/sdxl-base-1.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wiwOT0UZrkBi",
      "metadata": {
        "id": "wiwOT0UZrkBi"
      },
      "source": [
        "## STEP 4 ‚Äî Project Config\n",
        "\n",
        "In this step, we set up the project details and training parameters:  \n",
        "\n",
        "- **Project Name** ‚Äî used to create folders for datasets, outputs, and logs.  \n",
        "- **Trigger Word** ‚Äî a unique token (e.g., `akashawriter`) that activates your LoRA.  \n",
        "- **Paths** ‚Äî dataset, images, outputs, and logs are auto-created in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VGg2nGAcrkBj",
      "metadata": {
        "id": "VGg2nGAcrkBj"
      },
      "outputs": [],
      "source": [
        "# ==== USER CONFIG ====\n",
        "PROJECT_NAME = input(\"Please Enter Your Project Name: \").strip()\n",
        "TRIGGER      = input(\"Please Enter Your Trigger Word (Avoid Common Words): \").strip()\n",
        "\n",
        "# Paths\n",
        "DATASET_DIR  = f\"/content/{PROJECT_NAME}/\"\n",
        "IMAGES_DIR   = f\"/content/{PROJECT_NAME}/5_{TRIGGER}/\"\n",
        "OUTPUT_DIR   = f\"/content/drive/MyDrive/LoRA_Output/{PROJECT_NAME}\"\n",
        "LOG_DIR      = f\"/content/drive/MyDrive/LoRA_Logs/{PROJECT_NAME}\"\n",
        "\n",
        "# Create folders if not exist\n",
        "import os\n",
        "for p in [DATASET_DIR, OUTPUT_DIR, LOG_DIR, IMAGES_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Folders ready:\")\n",
        "print(\"üìÇ Dataset Dir :\", DATASET_DIR)\n",
        "print(\"üìÇ Images Dir  :\", IMAGES_DIR)\n",
        "print(\"üìÇ Output Dir  :\", OUTPUT_DIR)\n",
        "print(\"üìÇ Log Dir     :\", LOG_DIR)\n",
        "print(\"\\n‚ö° Place your training images inside:\", IMAGES_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "167xZLdZrkBk",
      "metadata": {
        "id": "167xZLdZrkBk"
      },
      "source": [
        "## STEP 5 ‚Äî Upload Images ( You Can Skip This If you Have DataSet Ready )\n",
        "### Option A ‚Äî Copy images into the dataset folder in **Google Drive** directly:\n",
        "- Put all images into: `LoRA_Datasets/5_<PROJECT_NAME>`\n",
        "\n",
        "### Option B ‚Äî Upload from your local machine (quick):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BZdHFosKrkBk",
      "metadata": {
        "id": "BZdHFosKrkBk"
      },
      "outputs": [],
      "source": [
        "# Upload images directly (if you didn't place them into Drive already)\n",
        "from google.colab import files\n",
        "print(\"Please Upload Your Training Data Photos: \")\n",
        "uploaded = files.upload()\n",
        "for fname, filedata in uploaded.items():\n",
        "    with open(os.path.join(IMAGES_DIR, fname), 'wb') as f:\n",
        "        f.write(filedata)\n",
        "print(\"Uploaded:\", list(uploaded.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y-XAC4Ub7a_v",
      "metadata": {
        "id": "Y-XAC4Ub7a_v"
      },
      "outputs": [],
      "source": [
        "# Converts Image to Single Format & Trims the photo\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Path where your images are stored\n",
        "# IMAGES_DIR = \"/content/drive/MyDrive/LoRA_Datasets/Vidya_Balan/10_Vidya_Balan\"\n",
        "\n",
        "TARGET_SIZE = 1024 # Chnage this as Required\n",
        "count = 0\n",
        "\n",
        "for fname in os.listdir(IMAGES_DIR):\n",
        "    fpath = os.path.join(IMAGES_DIR, fname)\n",
        "\n",
        "    # Skip non-image files\n",
        "    if not fname.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.avif')):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Load image with cv2 (handles webp/avif better than PIL directly)\n",
        "        img = cv2.imread(fpath, cv2.IMREAD_UNCHANGED)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è Skipped (not an image): {fname}\")\n",
        "            continue\n",
        "\n",
        "        # Convert to RGB\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        pil_img = Image.fromarray(img)\n",
        "\n",
        "        # --- Center crop (square) ---\n",
        "        w, h = pil_img.size\n",
        "        min_dim = min(w, h)\n",
        "        left = (w - min_dim) // 2\n",
        "        top = (h - min_dim) // 2\n",
        "        right = left + min_dim\n",
        "        bottom = top + min_dim\n",
        "        pil_img = pil_img.crop((left, top, right, bottom))\n",
        "\n",
        "        # --- Resize ---\n",
        "        pil_img = pil_img.resize((TARGET_SIZE, TARGET_SIZE), Image.LANCZOS)\n",
        "\n",
        "        # --- Save as JPG with clean name ---\n",
        "        clean_name = f\"vidya_{count:03d}.jpg\"\n",
        "        save_path = os.path.join(IMAGES_DIR, clean_name)\n",
        "        pil_img.save(save_path, \"JPEG\", quality=95)\n",
        "\n",
        "        # --- Delete the old file (only if name is different from the new one) ---\n",
        "        if fpath != save_path:\n",
        "            os.remove(fpath)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error on {fname}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Processed {count} images. Only clean JPG files remain in {IMAGES_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UaxPbyfnrkBl",
      "metadata": {
        "id": "UaxPbyfnrkBl"
      },
      "source": [
        "## STEP 6 ‚Äî (Optional) Auto‚ÄëGenerate Simple Captions\n",
        "Optional but Highly Reccomeded\n",
        "\n",
        "This creates a **.txt** file next to each image using your template. You can edit them later. For higher quality, you should hand‚Äëwrite short, accurate captions per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lvS-A4RBrkBl",
      "metadata": {
        "id": "lvS-A4RBrkBl"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "\n",
        "# Base caption template used for auto-captioning (optional). Keep it SIMPLE.\n",
        "# Adjust for your Images\n",
        "CAPTION_TEMPLATE = f\"photo of {TRIGGER}, professional portrait, high quality, detailed face, studio lighting\"\n",
        "\n",
        "print(\"After creating caption files, add the actual captions for accurate results. (Reccomended)\")\n",
        "image_exts = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "imgs = [p for p in glob.glob(os.path.join(DATASET_DIR, \"**/*\"), recursive=True) if os.path.splitext(p)[1].lower() in image_exts]\n",
        "print(f\"Found {len(imgs)} images\")\n",
        "\n",
        "for img_path in imgs:\n",
        "    base, _ = os.path.splitext(img_path)\n",
        "    txt_path = base + \".txt\"\n",
        "    if not os.path.exists(txt_path):\n",
        "        with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(CAPTION_TEMPLATE)\n",
        "\n",
        "print(\"Caption files created where missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Pwbo9farkBm",
      "metadata": {
        "id": "_Pwbo9farkBm"
      },
      "source": [
        "## STEP 7 ‚Äî Train LoRA (kohya-ss `sdxl_train_network.py`)\n",
        "**Tips:**\n",
        "- Start with the defaults. If results look samey/overfit, reduce `MAX_STEPS` or improve dataset.\n",
        "- If underfit (not learning your subject), **increase** `MAX_STEPS` to 6‚Äì8k or improve captions.\n",
        "- Keep `NETWORK_DIM` at 16/32 for small, flexible LoRAs.\n",
        "\n",
        "- **Resolution** ‚Äî choose `512` (faster, lighter) or `768` (more detail, higher VRAM).  \n",
        "- **Batch Size** ‚Äî set to `1` (use `2` if GPU has enough VRAM).  \n",
        "- **Max Steps** ‚Äî training iterations (start with 3k‚Äì5k, adjust based on results).  \n",
        "- **Network Dim / Alpha** ‚Äî controls LoRA size & capacity (16‚Äì32 is common).  \n",
        "- **Learning Rates** ‚Äî fine-tuned for text encoder and U-Net.  \n",
        "\n",
        "üëâ If you‚Äôre unsure, keep the defaults ‚Äî they work well for most cases.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gcL9qysdrkBm",
      "metadata": {
        "id": "gcL9qysdrkBm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ----------------------\n",
        "# CONFIG\n",
        "# ----------------------\n",
        "PRETRAINED_MODEL = MODEL_DIR\n",
        "\n",
        "# Hyperparameters tuned for A100/L4\n",
        "RESOLUTION        = 1024          # 768 or lesser is safe for Weaker GPU\n",
        "BATCH_SIZE        = 1             # Chnage it according to power\n",
        "GRAD_ACC_STEPS    = 2             # Chnage it according to power\n",
        "MAX_STEPS         = 6000\n",
        "NETWORK_DIM       = 64\n",
        "NETWORK_ALPHA     = 32\n",
        "LEARNING_RATE     = 1.0            # Prodigy expects ~1.0, not 1e-4\n",
        "\n",
        "# ----------------------\n",
        "# TRAINING COMMAND\n",
        "# ----------------------\n",
        "\n",
        "# --network_weights /content/drive/MyDrive/LoRA_Output/aigenmodel/500-*.safetensors\n",
        "\n",
        "train_cmd = f'''\n",
        "accelerate launch --mixed_precision=bf16 sdxl_train_network.py \\\n",
        "  --pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" \\\n",
        "  --train_data_dir=\"{DATASET_DIR}\" \\\n",
        "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "  --logging_dir=\"{LOG_DIR}\" \\\n",
        "  --resolution={RESOLUTION} \\\n",
        "  --network_module=networks.lora \\\n",
        "  --network_dim={NETWORK_DIM} \\\n",
        "  --network_alpha={NETWORK_ALPHA} \\\n",
        "  --learning_rate={LEARNING_RATE} \\\n",
        "  --train_batch_size={BATCH_SIZE} \\\n",
        "  --gradient_accumulation_steps={GRAD_ACC_STEPS} \\\n",
        "  --max_train_steps={MAX_STEPS} \\\n",
        "  --save_every_n_steps=500 \\\n",
        "  --save_last_n_steps=3 \\\n",
        "  --save_last_n_epochs=3 \\\n",
        "  --save_state \\\n",
        "  --save_precision=bf16 \\\n",
        "  --optimizer_type=Prodigy \\\n",
        "  --mem_eff_attn \\\n",
        "  --shuffle_caption \\\n",
        "  --caption_extension=.txt \\\n",
        "  --max_data_loader_n_workers=2 \\\n",
        "  --log_prefix=\"female_writer_v1\" \\\n",
        "  --enable_bucket \\\n",
        "  --bucket_reso_steps=64 \\\n",
        "  --random_crop \\\n",
        "  --log_with tensorboard \\\n",
        "  2>&1 | tee /content/train.log\n",
        "'''\n",
        "\n",
        "# ----------------------\n",
        "# RUN TRAINING\n",
        "# ----------------------\n",
        "print(\"üöÄ Starting LoRA training with Colab Pro A100/L4...\\n\")\n",
        "exit_code = os.system(train_cmd)\n",
        "print(\"\\n‚úÖ Training finished with exit code:\", exit_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xeBgZAS0rkBm",
      "metadata": {
        "id": "xeBgZAS0rkBm"
      },
      "source": [
        "## STEP 8 ‚Äî Test the LoRA (Diffusers)\n",
        "This loads SD1.5 and your LoRA, then generates a sample image.\n",
        "\n",
        "**Note:** If you downloaded SDXL in Step 3 (Option A), it will reuse that folder. If you used a `.safetensors` base model, you can still test with the diffusers SD1.5 pipeline below. (From the last Version of this Code i.e v0.1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xNDDm4RFrkBn",
      "metadata": {
        "id": "xNDDm4RFrkBn"
      },
      "outputs": [],
      "source": [
        "# A 24 GB VRAM L4 GPU May Also Work for This\n",
        "\n",
        "import os, glob\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "\n",
        "OUTPUT_DIR = r\"/content/drive/MyDrive/LoRA_Output/aigenmodel/\"\n",
        "RESOLUTION = 1024\n",
        "\n",
        "# find latest safetensors in output\n",
        "lora_files = sorted([p for p in glob.glob(os.path.join(OUTPUT_DIR, \"*.safetensors\"))], key=os.path.getmtime)\n",
        "assert lora_files, \"No LoRA files found in OUTPUT_DIR. Check training output.\"\n",
        "LORA_PATH = lora_files[-1]\n",
        "print(\"Using LoRA:\", LORA_PATH)\n",
        "\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Load LoRA\n",
        "pipe.load_lora_weights(os.path.dirname(LORA_PATH), weight_name=os.path.basename(LORA_PATH))\n",
        "\n",
        "while True:\n",
        "  print(\"Press Clt + C To Exit\")\n",
        "  # prompt = f\"portrait of {TRIGGER}, professional lighting, ultra-detailed, 8k, sharp focus\"\n",
        "  prompt = input(\"Build Anything (Make sure to add your Trigger word): \")\n",
        "  # neg = \"low quality, blurry, lowres, bad hands, worst quality, jpeg artifacts\"\n",
        "  neg = input(\"Negative Prompt (leave blank if none): \")\n",
        "  image = pipe(prompt, negative_prompt=neg, num_inference_steps=30, guidance_scale=7.5, height=RESOLUTION, width=RESOLUTION).images[0]\n",
        "\n",
        "  image.save(\"/content/sample_lora_output.png\")\n",
        "  print(\"Saved /content/sample_lora_output.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Do09cXMrkBn",
      "metadata": {
        "id": "8Do09cXMrkBn"
      },
      "source": [
        "## STEP 9 ‚Äî Download / Deliverables\n",
        "- Your LoRA `.safetensors` is saved in: `LoRA_Output/<PROJECT_NAME>` (on Drive).\n",
        "- The sample output image is at: `/content/sample_lora_output.png`.\n",
        "- You can zip the output folder for delivery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IOl1BTtDrkBn",
      "metadata": {
        "id": "IOl1BTtDrkBn"
      },
      "outputs": [],
      "source": [
        "# Take back What you Trained\n",
        "\n",
        "!zip -r /content/lora_output.zip \"$OUTPUT_DIR\" || echo \"Zip failed (likely no files).\"\n",
        "print(\"If succeeded, download: /content/lora_output.zip from Colab sidebar ‚Üí Files.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}