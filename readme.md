# ğŸ”§ LoRA Training â€” Stable Diffusion 1.5 (Google Colab)

This repository contains a **Google Colab notebook** to train **LoRA models** on **Stable Diffusion 1.5**.  
It is designed for creators who want to fine-tune SD1.5 on custom datasets (faces, characters, styles, etc.).  

---

## ğŸš€ Features
- âœ… Automatic environment setup (Python 3.10, CUDA 12.1, Torch 2.2.2)  
- âœ… Tested with **kohya-ss sd-scripts**  
- âœ… Drive integration for dataset, cache, outputs, and logs  
- âœ… Custom **trigger token** for your LoRA  
- âœ… Configurable hyperparameters (resolution, batch size, steps, etc.)  
- âœ… Optimized with **xformers** and **bitsandbytes**  

---

## ğŸ“‚ Notebook Sections
1. **Environment Setup** â€” GPU check, Drive mount, Python 3.10 upgrade  
2. **Dependency Installation** â€” Torch + CUDA, Hugging Face stack, utilities  
3. **Dataset Preparation** â€” Split into close-ups, medium, full-body  
4. **Project Config** â€” Define project name, trigger word, training parameters  
5. **Training** â€” Launch kohya-ss script with your settings  
6. **Export & Inference** â€” Save LoRA to Drive, test with prompt  

---

## âš™ï¸ Project Config (STEP 4)
When configuring your project in the notebook, set:

- **Project Name** â†’ folder for datasets, outputs, logs  
- **Trigger Word** â†’ unique keyword (e.g. `akashawriter`)  
- **Resolution** â†’ `512` (lighter/faster) or `768` (better detail, heavy VRAM)  
- **Batch Size** â†’ `1` for T4 GPU, `2+` if you have higher VRAM  
- **Max Steps** â†’ start with `3000â€“5000` for decent results  
- **Network Dim / Alpha** â†’ `16` or `32` (higher = more capacity, heavier model)  
- **Learning Rate** â†’ Default `1e-4` works for most cases  

ğŸ‘‰ If unsure, stick to defaults.  

---

## ğŸ”¥ Best Settings for Maximum Quality
If you want **the best possible output**, even if it consumes more GPU memory or takes longer:  

- **Resolution** â†’ `768`  
- **Batch Size** â†’ `2` (if VRAM allows)  
- **Max Steps** â†’ `8000â€“12000` (longer training = sharper features, but risk of overfit)  
- **Network Dim** â†’ `32` (more detail capacity)  
- **Learning Rate** â†’ keep defaults; donâ€™t go too high or it overfits  
- **Dataset** â†’  
  - 60â€“70% close-up faces  
  - 20â€“30% medium shots  
  - 10â€“20% full body  
  - Use **high-res, diverse images** with consistent labeling  
  - Avoid duplicates and blurred/noisy images  

âš ï¸ Note: On Colab T4 GPUs, training with these settings may take several hours. For faster results, reduce steps.  

---

## ğŸ“Š Tips for Best Results
- Clean your dataset manually before training (blurred/duplicate images kill quality).  
- Use **unique, uncommon trigger tokens** (avoid generic words).  
- Save intermediate checkpoints (e.g., at 3k, 6k, 10k steps).  
- Test LoRA at multiple checkpoints â€” sometimes earlier ones perform better.  

---

## ğŸ“œ License
This notebook is based on [kohya-ss sd-scripts](https://github.com/kohya-ss/sd-scripts).  
Follow the license of that repository when redistributing.  

---

## ğŸ™Œ Credits
- Stable Diffusion 1.5 by [CompVis & StabilityAI](https://github.com/CompVis/stable-diffusion)  
- LoRA training scripts from [kohya-ss](https://github.com/kohya-ss/sd-scripts)  
- Colab adaptation by Akash  
