{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YhRzp4wwrkBZ",
      "metadata": {
        "id": "YhRzp4wwrkBZ"
      },
      "source": [
        "# üîß Female LoRA Training (Stable Diffusion 1.5) ‚Äî Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step - 1 : Environment Setup ‚Äî GPU (T4/A100 Reccomemded) & Drive Mount\n",
        "\n",
        "Before installing dependencies, we need to:  \n",
        "\n",
        "- **Check GPU** ‚Äî make sure Colab is running with a GPU.  \n",
        "- **Mount Google Drive** ‚Äî to save datasets, models, and outputs.  \n",
        "- **Set cache dirs** ‚Äî reuse Hugging Face/pip downloads for faster runs.  \n",
        "- **Upgrade Python** ‚Äî switch Colab from 3.9 ‚Üí 3.10 (required for kohya-ss).  \n"
      ],
      "metadata": {
        "id": "tHs1GHFpa-tZ"
      },
      "id": "tHs1GHFpa-tZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k6nzBB_mrkBd",
      "metadata": {
        "id": "k6nzBB_mrkBd"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi || echo \"No GPU detected ‚Äî set Colab runtime to GPU.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kY96rsKYrkBf",
      "metadata": {
        "id": "kY96rsKYrkBf"
      },
      "outputs": [],
      "source": [
        "# STEP 1 ‚Äî Mount Google Drive (stores datasets/models/outputs)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LBXR82cR_g_K",
      "metadata": {
        "id": "LBXR82cR_g_K"
      },
      "outputs": [],
      "source": [
        "# Save Catch to GDrive\n",
        "%env HF_HOME=/content/drive/MyDrive/hf_cache\n",
        "%env TRANSFORMERS_CACHE=/content/drive/MyDrive/hf_cache\n",
        "%env HF_DATASETS_CACHE=/content/drive/MyDrive/hf_cache\n",
        "%env PIP_CACHE_DIR=/content/drive/MyDrive/pip_cache"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "id": "raoAC6Z-htFt"
      },
      "id": "raoAC6Z-htFt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Force Colab to use Python 3.10\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y python3.10 python3.10-dev python3.10-distutils\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "!sudo update-alternatives --config python3\n",
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10\n",
        "!python3 --version"
      ],
      "metadata": {
        "id": "DnRV_ZQmhwku"
      },
      "id": "DnRV_ZQmhwku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version Must Be 3.10\n",
        "!python3 --version"
      ],
      "metadata": {
        "id": "_sn4iTG8n_l8"
      },
      "id": "_sn4iTG8n_l8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step - 2 : Dependency Installation Steps\n",
        "\n",
        "Before training a LoRA, we need to prepare the Colab environment.  \n",
        "By default, Colab ships with older Python and mismatched CUDA/PyTorch versions, which can cause errors.  \n",
        "The following steps will:  \n",
        "\n",
        "- **Force Colab to use Python 3.10** (required for kohya-ss scripts).  \n",
        "- **Install the correct PyTorch + CUDA 12.1 stack** (works well with T4 GPUs).  \n",
        "- **Add xformers and bitsandbytes** for efficient memory usage and 8-bit optimizers.  \n",
        "- **Install Hugging Face + diffusers libraries** (specific versions tested for stability).  \n",
        "- **Include extra utilities** for training, logging, and image handling.  \n",
        "- **Finally, verify all versions** to ensure the setup is correct.  \n"
      ],
      "metadata": {
        "id": "zBbv3SYMYewa"
      },
      "id": "zBbv3SYMYewa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch + CUDA 12.1 stack (stable for T4, SD 1.5 training)\n",
        "!pip install -q torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "yHIjbAzilWJg"
      },
      "id": "yHIjbAzilWJg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xformers (must match torch), bitsandbytes (for 8-bit optimizer)\n",
        "!pip install -q xformers==0.0.25.post1 bitsandbytes==0.43.1"
      ],
      "metadata": {
        "id": "bQhLw8LTlY8J"
      },
      "id": "bQhLw8LTlY8J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face + diffusers stack (versions tested with kohya-ss)\n",
        "!pip install -q accelerate==0.27.2 transformers==4.39.3 diffusers==0.25.0 safetensors==0.4.2"
      ],
      "metadata": {
        "id": "xkqEGzsClbH3"
      },
      "id": "xkqEGzsClbH3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  einops==0.7.0 ftfy==6.1.1 tensorboard==2.17.0 \\\n",
        "  opencv-python==4.8.1.78 pillow tqdm sentencepiece datasets==2.19.0 \\\n",
        "  pytorch-lightning==1.9.0 prodigyopt==1.0 lion-pytorch==0.0.6 \\\n",
        "  altair==4.2.2 easygui==0.98.3 toml==0.10.2 voluptuous==0.13.1 \\\n",
        "  huggingface-hub==0.24.5 imagesize==1.4.1 rich==13.7.0"
      ],
      "metadata": {
        "id": "49EbDvddleFd"
      },
      "id": "49EbDvddleFd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy==1.26.4"
      ],
      "metadata": {
        "id": "RZ04bkJ3nL8s"
      },
      "id": "RZ04bkJ3nL8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fresh clone of sd-scripts\n",
        "%cd /content/\n",
        "!rm -rf sd-scripts\n",
        "!git clone https://github.com/kohya-ss/sd-scripts.git\n",
        "%cd sd-scripts"
      ],
      "metadata": {
        "id": "fxMSVRpGlgt0"
      },
      "id": "fxMSVRpGlgt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -c \"import torch; print('Torch:', torch.__version__, '| CUDA:', torch.version.cuda, '| GPU OK:', torch.cuda.is_available())\"\n",
        "!python3 -c \"import xformers; print('Xformers:', xformers.__version__)\"\n",
        "!python3 -c \"import diffusers; print('Diffusers:', diffusers.__version__)\"\n",
        "!python3 -c \"import transformers; print('Transformers:', transformers.__version__)\"\n",
        "!python3 -c \"import accelerate; print('Accelerate:', accelerate.__version__)\""
      ],
      "metadata": {
        "id": "L3dXhmXElirb"
      },
      "id": "L3dXhmXElirb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "TGjFh3XTrkBh",
      "metadata": {
        "id": "TGjFh3XTrkBh"
      },
      "source": [
        "## STEP 3 ‚Äî Choose Base Model\n",
        "You have **two options**:\n",
        "1. **Download SD1.5 from Hugging Face** (requires a free account + accepted license). Recommended for first-time users.\n",
        "2. **Point to a local `.safetensors`** you already have in Drive.\n",
        "\n",
        "‚ö†Ô∏è **SD1.5** is lighter and easier to train than SDXL. Use SD1.5 unless you **know** you need SDXL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sa-Y_xz6rkBi",
      "metadata": {
        "id": "Sa-Y_xz6rkBi"
      },
      "outputs": [],
      "source": [
        "# OPTION A ‚Äî Download SD1.5 (requires Hugging Face token)\n",
        "USE_HF = True  # set False if you want to use a local .safetensors instead\n",
        "MODEL_REPO = \"runwayml/stable-diffusion-v1-5\"  # SD1.5 official repo\n",
        "MODEL_DIR = \"/content/drive/MyDrive/SD/models/sd15\"\n",
        "\n",
        "if USE_HF:\n",
        "    from huggingface_hub import login, snapshot_download\n",
        "    print(\"üîê Login to Hugging Face (paste your access token):\")\n",
        "    login()\n",
        "    snapshot_download(MODEL_REPO, local_dir=MODEL_DIR, local_dir_use_symlinks=False)\n",
        "    BASE_MODEL_PATH = MODEL_DIR\n",
        "else:\n",
        "    # OPTION B ‚Äî Use a local .safetensors file stored on Drive\n",
        "    BASE_MODEL_PATH = \"/content/drive/MyDrive/SD/models/sd15.safetensors\"\n",
        "\n",
        "print(\"BASE_MODEL_PATH:\", BASE_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wiwOT0UZrkBi",
      "metadata": {
        "id": "wiwOT0UZrkBi"
      },
      "source": [
        "## STEP 4 ‚Äî Project Config\n",
        "\n",
        "In this step, we set up the project details and training parameters:  \n",
        "\n",
        "- **Project Name** ‚Äî used to create folders for datasets, outputs, and logs.  \n",
        "- **Trigger Word** ‚Äî a unique token (e.g., `akashawriter`) that activates your LoRA.  \n",
        "- **Paths** ‚Äî dataset, images, outputs, and logs are auto-created in Google Drive.  \n",
        "- **Resolution** ‚Äî choose `512` (faster, lighter) or `768` (more detail, higher VRAM).  \n",
        "- **Batch Size** ‚Äî set to `1` (use `2` if GPU has enough VRAM).  \n",
        "- **Max Steps** ‚Äî training iterations (start with 3k‚Äì5k, adjust based on results).  \n",
        "- **Network Dim / Alpha** ‚Äî controls LoRA size & capacity (16‚Äì32 is common).  \n",
        "- **Learning Rates** ‚Äî fine-tuned for text encoder and U-Net.  \n",
        "\n",
        "üëâ If you‚Äôre unsure, keep the defaults ‚Äî they work well for most cases.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VGg2nGAcrkBj",
      "metadata": {
        "id": "VGg2nGAcrkBj"
      },
      "outputs": [],
      "source": [
        "# ==== USER CONFIG ====\n",
        "PROJECT_NAME = input(\"Please Enter Your Project Name: \")\n",
        "TRIGGER      = input(\"Please Enter Your Trigger Word (Avoid Common Words): \")\n",
        "DATASET_DIR  = f\"/content/drive/MyDrive/LoRA_Datasets/{PROJECT_NAME}\"\n",
        "IMAGES_DIR  = f\"/content/drive/MyDrive/LoRA_Datasets/{PROJECT_NAME}/10_{TRIGGER}/\"\n",
        "OUTPUT_DIR   = f\"/content/drive/MyDrive/LoRA_Output/{PROJECT_NAME}\"\n",
        "LOG_DIR      = f\"/content/drive/MyDrive/LoRA_Logs/{PROJECT_NAME}\"\n",
        "\n",
        "RESOLUTION        = 768   # 512 or 768; 768 gives more detail if VRAM allows\n",
        "BATCH_SIZE        = 1     # increase to 2 if GPU VRAM allows\n",
        "MAX_STEPS         = 4000  # start with 3‚Äì5k; iterate based on results\n",
        "NETWORK_DIM       = 16    # 16/32 are good starting points; higher = heavier model\n",
        "NETWORK_ALPHA     = 16\n",
        "LEARNING_RATE     = 0.0001\n",
        "TEXT_ENCODER_LR   = 5e-5\n",
        "UNET_LR           = 1e-4\n",
        "\n",
        "import os\n",
        "for p in [DATASET_DIR, OUTPUT_DIR, LOG_DIR, IMAGES_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "print(\"Folders ready:\\n\", DATASET_DIR, \"\\n\", OUTPUT_DIR, \"\\n\", LOG_DIR, \"\\n\", IMAGES_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "167xZLdZrkBk",
      "metadata": {
        "id": "167xZLdZrkBk"
      },
      "source": [
        "## STEP 5 ‚Äî Upload Images\n",
        "### Option A ‚Äî Copy images into the dataset folder in **Google Drive** directly:\n",
        "- Put all images into: `LoRA_Datasets/<PROJECT_NAME>`\n",
        "\n",
        "### Option B ‚Äî Upload from your local machine (quick):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BZdHFosKrkBk",
      "metadata": {
        "id": "BZdHFosKrkBk"
      },
      "outputs": [],
      "source": [
        "# Upload images directly (if you didn't place them into Drive already)\n",
        "from google.colab import files\n",
        "print(\"Please Upload Your Training Data Photos: \")\n",
        "uploaded = files.upload()\n",
        "for fname, filedata in uploaded.items():\n",
        "    with open(os.path.join(IMAGES_DIR, fname), 'wb') as f:\n",
        "        f.write(filedata)\n",
        "print(\"Uploaded:\", list(uploaded.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UaxPbyfnrkBl",
      "metadata": {
        "id": "UaxPbyfnrkBl"
      },
      "source": [
        "## STEP 6 ‚Äî (Optional) Auto‚ÄëGenerate Simple Captions\n",
        "This creates a **.txt** file next to each image using your template. You can edit them later. For higher quality, you should hand‚Äëwrite short, accurate captions per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lvS-A4RBrkBl",
      "metadata": {
        "id": "lvS-A4RBrkBl"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "\n",
        "# Base caption template used for auto-captioning (optional). Keep it SIMPLE.\n",
        "# Adjust for your Images\n",
        "CAPTION_TEMPLATE = f\"photo of {TRIGGER}, professional portrait, studio lighting, high detail\"\n",
        "\n",
        "print(\"After creating caption files, add the actual captions for accurate results. (Reccomended)\")\n",
        "image_exts = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "imgs = [p for p in glob.glob(os.path.join(DATASET_DIR, \"**/*\"), recursive=True) if os.path.splitext(p)[1].lower() in image_exts]\n",
        "print(f\"Found {len(imgs)} images\")\n",
        "\n",
        "for img_path in imgs:\n",
        "    base, _ = os.path.splitext(img_path)\n",
        "    txt_path = base + \".txt\"\n",
        "    if not os.path.exists(txt_path):\n",
        "        with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(CAPTION_TEMPLATE)\n",
        "\n",
        "print(\"Caption files created where missing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Pwbo9farkBm",
      "metadata": {
        "id": "_Pwbo9farkBm"
      },
      "source": [
        "## STEP 7 ‚Äî Train LoRA (kohya-ss `train_network.py`)\n",
        "**Tips:**\n",
        "- Start with the defaults. If results look samey/overfit, reduce `MAX_STEPS` or improve dataset.\n",
        "- If underfit (not learning your subject), **increase** `MAX_STEPS` to 6‚Äì8k or improve captions.\n",
        "- Keep `NETWORK_DIM` at 16/32 for small, flexible LoRAs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gcL9qysdrkBm",
      "metadata": {
        "id": "gcL9qysdrkBm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PRETRAINED_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Training command\n",
        "train_cmd = f'''\n",
        "accelerate launch train_network.py \\\n",
        "  --pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" \\\n",
        "  --train_data_dir=\"{DATASET_DIR}\" \\\n",
        "  --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "  --logging_dir=\"{LOG_DIR}\" \\\n",
        "  --resolution={RESOLUTION} \\\n",
        "  --network_module=networks.lora \\\n",
        "  --network_dim={NETWORK_DIM} \\\n",
        "  --network_alpha={NETWORK_ALPHA} \\\n",
        "  --learning_rate={LEARNING_RATE} \\\n",
        "  --text_encoder_lr={TEXT_ENCODER_LR} \\\n",
        "  --unet_lr={UNET_LR} \\\n",
        "  --train_batch_size={BATCH_SIZE} \\\n",
        "  --max_train_steps={MAX_STEPS}} \\\n",
        "  --save_every_n_steps=200 \\\n",
        "  --mixed_precision=fp16 \\\n",
        "  --save_precision=fp16 \\\n",
        "  --optimizer_type=AdamW8bit \\\n",
        "  --xformers \\\n",
        "  --shuffle_caption \\\n",
        "  --caption_extension=.txt \\\n",
        "  --max_data_loader_n_workers=1 \\\n",
        "  --clip_skip=2 \\\n",
        "  --log_prefix=\"female_writer_v1\" \\\n",
        "  --enable_bucket \\\n",
        "  --bucket_reso_steps=64 \\\n",
        "  --random_crop \\\n",
        "  2>&1 | tee /content/train.log\n",
        "'''\n",
        "\n",
        "# Run training\n",
        "print(\"Starting LoRA training...\\n\")\n",
        "exit_code = os.system(train_cmd)\n",
        "print(\"\\nTraining finished with exit code:\", exit_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xeBgZAS0rkBm",
      "metadata": {
        "id": "xeBgZAS0rkBm"
      },
      "source": [
        "## STEP 8 ‚Äî Test the LoRA (Diffusers)\n",
        "This loads SD1.5 and your LoRA, then generates a sample image.\n",
        "\n",
        "**Note:** If you downloaded SD1.5 in Step 3 (Option A), it will reuse that folder. If you used a `.safetensors` base model, you can still test with the diffusers SD1.5 pipeline below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xNDDm4RFrkBn",
      "metadata": {
        "id": "xNDDm4RFrkBn"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# find latest safetensors in output\n",
        "lora_files = sorted([p for p in glob.glob(os.path.join(OUTPUT_DIR, \"*.safetensors\"))], key=os.path.getmtime)\n",
        "assert lora_files, \"No LoRA files found in OUTPUT_DIR. Check training output.\"\n",
        "LORA_PATH = lora_files[-1]\n",
        "print(\"Using LoRA:\", LORA_PATH)\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "try:\n",
        "    pipe.load_lora_weights(os.path.dirname(LORA_PATH), weight_name=os.path.basename(LORA_PATH))\n",
        "    print(\"LoRA loaded via Diffusers API.\")\n",
        "except Exception as e:\n",
        "    print(\"LoRA load failed:\", e)\n",
        "\n",
        "# prompt = f\"portrait of {TRIGGER}, professional lighting, ultra-detailed, 8k, sharp focus\"\n",
        "prompt = input(\"Build Anything (Make sure to add your Trigger word): \")\n",
        "# neg = \"low quality, blurry, lowres, bad hands, worst quality, jpeg artifacts\"\n",
        "neg = input(\"Negative Prompt (leave blank if none): \")\n",
        "image = pipe(prompt, negative_prompt=neg, num_inference_steps=30, guidance_scale=7.5, height=RESOLUTION, width=RESOLUTION).images[0]\n",
        "\n",
        "image.save(\"/content/sample_lora_output.png\")\n",
        "print(\"Saved /content/sample_lora_output.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Do09cXMrkBn",
      "metadata": {
        "id": "8Do09cXMrkBn"
      },
      "source": [
        "## STEP 9 ‚Äî Download / Deliverables\n",
        "- Your LoRA `.safetensors` is saved in: `LoRA_Output/<PROJECT_NAME>` (on Drive).\n",
        "- The sample output image is at: `/content/sample_lora_output.png`.\n",
        "- You can zip the output folder for delivery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IOl1BTtDrkBn",
      "metadata": {
        "id": "IOl1BTtDrkBn"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/lora_output.zip \"$OUTPUT_DIR\" || echo \"Zip failed (likely no files).\"\n",
        "print(\"If succeeded, download: /content/lora_output.zip from Colab sidebar ‚Üí Files.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}